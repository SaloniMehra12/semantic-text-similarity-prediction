# semantic-text-similarity-prediction
semantic text similarity prediction
Model 2 =>TF-IDF APPROACH
REASON:
a.tf-idf is a weighting scheme that assigns each term in a document a weight based on its term frequency (tf) and inverse document frequency (idf). The terms with higher weight scores are considered to be more important 
b.use of word.net which helps in determining synonyms with reference to context which helps in determining meaning of content
c.two documents can be called similar If words occurrence are same in both the content 
d.cosine formulation used to determine similarity function because it lies in 0 and 1 range with lemmas
e.Method used=clustering,unsupervised learning
f.Fast in process
explanation:
TF(w) = (Number of times term w appears in a document) / (Total number of terms in the document)
IDF(w) = log_e(Total number of documents / Number of documents with term w in it)

 <img src="https://qphs.fs.quoracdn.net/main-qimg-2974121610a328de406587ebbe18649d" width="300"> 
 <img src="https://qphs.fs.quoracdn.net/main-qimg-62f658d0d3398d5128be415b4b4a49d6" width="300"> 
 <img src="https://qphs.fs.quoracdn.net/main-qimg-60a54c42850675139e2899634d3a669c" width="300">

Cosine Similarity (d1, d2) =  Dot product(d1, d2) / ||d1|| * ||d2||

Dot product (d1,d2) = d1[0] * d2[0] + d1[1] * d2[1] * … * d1[n] * d2[n]
||d1|| = square root(d1[0]2 + d1[1]2 + ... + d1[n]2)
||d2|| = square root(d2[0]2 + d2[1]2 + ... + d2[n]2)

<br>

Model 2 =>Doc2Vec APPROACH
REASON 
a)Easy analysis of documents 
b)Less processing and code
c)Automated model available in nltk
d)More accurate similarity
and
<img src="https://ai.intelligentonlinetools.com/ml/wp-content/uploads/2018/09/embeddings-e1538523583798.png" width="300"><br>
Explanation:

a.Doc2vec is an unsupervised computer algorithm to generate vectors for sentence/paragraphs/documents. The algorithm is an adaptation of word2vec which can generate vectors for words. 
b.The vectors generated by doc2vec used for tasks like finding similarity between sentences
c.the paragraph vector was added to represent the missing information from the current context and to act as a memory of the topic of the paragraph
d.Uses cluster method but with some labeling to distinguish two texts and make comparison 
e.Word embedding method is used to make data vectors

